{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ff9d51-2697-4c89-a397-f8328906cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37947f8-1800-457d-b80d-96f0f86b7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551b2cdb-71a8-4ac5-9727-6ae8fbca351c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_no_accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chây ì nộp phạt nguội</td>\n",
       "      <td>chay i nop phat nguoi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cháu đòi tiền cơm dì đòi tiền nhà</td>\n",
       "      <td>chau doi tien com di doi tien nha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đà nẵng nghiên cứu tiện ích nhắn tin khi vi ph...</td>\n",
       "      <td>da nang nghien cuu tien ich nhan tin khi vi ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khó xử vụ mẹ tuổi trộm xe hơi của con gái</td>\n",
       "      <td>kho xu vu me tuoi trom xe hoi cua con gai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thay đổi về đăng ký chuyển nhượng xe từ bạn cầ...</td>\n",
       "      <td>thay doi ve dang ky chuyen nhuong xe tu ban ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  \\\n",
       "0                              chây ì nộp phạt nguội   \n",
       "1                  cháu đòi tiền cơm dì đòi tiền nhà   \n",
       "2  đà nẵng nghiên cứu tiện ích nhắn tin khi vi ph...   \n",
       "3          khó xử vụ mẹ tuổi trộm xe hơi của con gái   \n",
       "4  thay đổi về đăng ký chuyển nhượng xe từ bạn cầ...   \n",
       "\n",
       "                                text_clean_no_accent  \n",
       "0                              chay i nop phat nguoi  \n",
       "1                  chau doi tien com di doi tien nha  \n",
       "2  da nang nghien cuu tien ich nhan tin khi vi ph...  \n",
       "3          kho xu vu me tuoi trom xe hoi cua con gai  \n",
       "4  thay doi ve dang ky chuyen nhuong xe tu ban ca...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e060c27-29ac-4e06-b58c-05b34122ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize by space\n",
    "tokenizer = get_tokenizer(tokenizer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c078a0a9-5be8-4cc3-a9ad-c41e55b80018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(iter_text_data, tokenizer):\n",
    "    counter = Counter()\n",
    "    for line in iter_text_data:#df.text_clean.to_numpy():\n",
    "        counter.update(tokenizer(line))\n",
    "    return Vocab(counter, min_freq=8, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06059904-6b2b-4b0f-8cc3-a0c7aeebe909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 240 ms, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tone_vocab = build_vocab(df.text_clean.to_numpy(), tokenizer)\n",
    "no_tone_vocab = build_vocab(df.text_clean_no_accent.to_numpy(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5307adba-8c62-46b1-9651-f3f23e442ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 82.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = np.array([f for w, f in tone_vocab.freqs.most_common()])\n",
    "# plt.hist(freq)\n",
    "from scipy.stats import iqr\n",
    "# iqr(freq, rng=(25, 75))\n",
    "np.percentile(freq, [10 ,90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef6f0a2-05f5-4127-bc5b-848c0e54dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(tone_array, no_tone_array, tone_vocab, no_tone_vocab, tokenizer):\n",
    "    data = []\n",
    "    for (tone_str, no_tone_str) in tqdm(zip(tone_array, no_tone_array)):\n",
    "#         print(tone_str, no_tone_str)\n",
    "#         break\n",
    "        tone_tensor_ = torch.tensor([tone_vocab[token] for token in tokenizer(tone_str)],\n",
    "                                dtype=torch.long)\n",
    "        no_tone_tensor_ = torch.tensor([no_tone_vocab[token] for token in tokenizer(no_tone_str)],\n",
    "                                dtype=torch.long)\n",
    "        data.append((tone_tensor_, no_tone_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "112c04ab-fe4b-485a-862f-b57fbc4ba5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (100, 2))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_range = 1000#int(df.shape[0]*1/100)\n",
    "test_range = 100#int(df.shape[0]*99/100)\n",
    "df_train = df.iloc[:train_range, :]\n",
    "df_test = df.iloc[-test_range:, :]\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00d08316-db45-4798-ad56-cedb64c14329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29246, 23902)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tone_vocab), len(no_tone_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb648928-7b79-429e-9975-db1ac60861fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 5960.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "train_data = data_process(df_train.text_clean.to_numpy(), \n",
    "                          df_train.text_clean_no_accent.to_numpy(), \n",
    "                          tone_vocab, no_tone_vocab, tokenizer)\n",
    "# val_data = data_process(df_test.text_clean.to_numpy(), \n",
    "#                           df_test.text_clean_no_accent.to_numpy(), \n",
    "#                           tone_vocab, no_tone_vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07f0ab11-1bf2-4457-aea8-260d81f3b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1552.87it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = data_process(df_test.text_clean.to_numpy(), \n",
    "                          df_test.text_clean_no_accent.to_numpy(), \n",
    "                          tone_vocab, no_tone_vocab, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa296c1b-6c5f-482d-81fb-0dce477d4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fed23a2-2455-4c7d-b717-c1d1541ac186",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = tone_vocab['<pad>']\n",
    "BOS_IDX = tone_vocab['<bos>']\n",
    "EOS_IDX = tone_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    tone_batch, no_tone_batch = [], []\n",
    "    for (tone_item, no_tone_item) in data_batch:\n",
    "        tone_batch.append(torch.cat([torch.tensor([BOS_IDX]), tone_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        no_tone_batch.append(torch.cat([torch.tensor([BOS_IDX]), no_tone_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        \n",
    "    tone_batch = pad_sequence(tone_batch, padding_value=PAD_IDX)\n",
    "    no_tone_batch = pad_sequence(no_tone_batch, padding_value=PAD_IDX)\n",
    "    return no_tone_batch, tone_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cb2c1d8-7b36-46c6-bcc7-0cdcd7eb9a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.4 s, sys: 5.58 ms, total: 3.4 s\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "826ab8b4-c220-4ef6-942f-0559ae85fb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 128]) torch.Size([25, 128])\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "    print(X.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e490f9aa-7157-417c-8d13-bb6640be0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embed_size: int,\n",
    "                 num_hiddens: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                X: Tensor) -> Tuple[Tensor]:\n",
    "        # X: (batch_size, num_steps)\n",
    "        X = self.embedding(X)\n",
    "        # X: (batch_size, num_steps, vocab_size)\n",
    "        # swap time steps axis \n",
    "        X = X.permute(1, 0, 2)\n",
    "        # X: (num_steps, batch_size, vocab_size)\n",
    "        \n",
    "        output, state = self.rnn(X)\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39ee6194-c425-4b28-b642-4be4f904be83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 1]), torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(10, 5, 1, 2, 0.1)\n",
    "X = torch.ones((3, 5), dtype=torch.long)\n",
    "y, state = encoder(X)\n",
    "y.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9b16a70-edc5-4428-a73f-440857a8ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embed_size: int,\n",
    "                 num_hiddens: int,\n",
    "                 num_layers: int,\n",
    "                 dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    \n",
    "    def init_state(self, enc_outputs):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def forward(self,\n",
    "                X: Tensor,\n",
    "                state: Tensor) -> Tuple[Tensor]:\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        \n",
    "        # state: (num_layers, num_batch, num_hiddens)\n",
    "        # broadcast context to perform broadcast sum: (num_steps, 1, 1)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        \n",
    "        X_context = torch.cat([X, context], dim=2)\n",
    "        output, state = self.rnn(X_context, state)\n",
    "        # output (num_steps, batch_size, num_hiddens) -> (num_steps, batch_size, vocab_size)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dae93ae9-07f3-4c1b-b0e7-3d5f77ffced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 10]), torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(10, 5, 1, 2, 0.0)\n",
    "X = torch.ones((3, 5), dtype=torch.long)\n",
    "state = decoder.init_state(encoder(X))\n",
    "y, state = decoder(X, state)\n",
    "y.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d7fa37d-1e6c-4edf-b9d9-899a29ffa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: nn.Module,\n",
    "                 decoder: nn.Module,\n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self,\n",
    "                enc_X: Tensor,\n",
    "                dec_X: Tensor) -> Tensor:\n",
    "        enc_outputs = self.encoder(enc_X)\n",
    "        dec_state = self.decoder.init_state(enc_outputs)\n",
    "        \n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09642136-5458-41bd-8023-937bbcb9aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, no_tone_vocab, device):\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    PAD_IDX = tone_vocab.stoi['<pad>']\n",
    "    loss = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            X, Y = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([no_tone_vocab['<bos>']]*Y.shape[0], device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1) # teacher forcing\n",
    "            \n",
    "            Y_hat, _ = net(X, dec_input)\n",
    "            \n",
    "            l = loss(Y_hat.permute(0, 2, 1), Y)#nn.functional.one_hot(Y, len(no_tone_vocab)).squeeze())\n",
    "            \n",
    "            l.sum().backward()\n",
    "            \n",
    "            # clip gradient\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            epoch_loss += l.item()\n",
    "        if ((epoch + 1) % 50 == 0):\n",
    "            print(f'Loss {epoch_loss/len(data_iter):.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c179abe7-edd0-4bbc-971d-fa66b7d20c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_vocab_size = len(tone_vocab)\n",
    "no_tone_vocab_size = len(no_tone_vocab)\n",
    "# ENC_EMB_DIM = 256\n",
    "# DEC_EMB_DIM = 256\n",
    "# ENC_HID_DIM = 512\n",
    "# DEC_HID_DIM = 512\n",
    "# ATTN_DIM = 64\n",
    "# ENC_DROPOUT = 0.5\n",
    "# DEC_DROPOUT = 0.5\n",
    "\n",
    "embed_size = 32\n",
    "num_hiddens = 64\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "lr = 0.05\n",
    "num_epochs = 10\n",
    "\n",
    "enc = Encoder(no_tone_vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "dec = Decoder(tone_vocab_size, embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "net = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ceebb05-9743-4d1a-84ee-c35e32bb1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,701,566 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(net):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d7c0b8f-9819-4a58-801c-0ab43e01021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:52<06:09, 52.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-d1d6e5355586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_tone_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-88484c11b060>\u001b[0m in \u001b[0;36mtrain_seq2seq\u001b[0;34m(net, data_iter, lr, num_epochs, no_tone_vocab, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#nn.functional.one_hot(Y, len(no_tone_vocab)).squeeze())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# clip gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_seq2seq(net, train_iter, lr, num_epochs, no_tone_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671e197-56d5-4cea-b744-73427c25faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, sentence, no_tone_vocab, tone_vocab, ):\n",
    "    net.eval()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
